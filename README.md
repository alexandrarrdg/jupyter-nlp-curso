# üéì Curso Completo de NLP y Modelos de Lenguaje (LLMs)

## üìö Descripci√≥n del Curso

Este repositorio contiene un curso universitario completo sobre **Procesamiento de Lenguaje Natural (NLP)** y **Modelos de Lenguaje de Gran Escala (LLMs)**, dise√±ado para estudiantes sin experiencia previa en el √°rea.

El curso est√° organizado en **tres Jupyter Notebooks interactivos** en espa√±ol, 100% ejecutables en Google Colab, que cubren desde conceptos fundamentales hasta la implementaci√≥n de sistemas avanzados con inteligencia artificial.

---

## üìñ Contenido del Curso

### üìì Notebook 1: Introducci√≥n a LLMs
**Archivo**: `Introduccion_a_LLM.ipynb`

Curso introductorio dise√±ado para estudiantes sin experiencia previa en NLP.

**Temas cubiertos**:
- ¬øQu√© son los LLMs y c√≥mo funcionan?
- Tokenizaci√≥n y procesamiento de texto
- Embeddings y representaciones vectoriales
- Similitud sem√°ntica
- Clasificaci√≥n de texto con transformers
- Generaci√≥n de texto
- Modelos multiling√ºes
- Fine-tuning b√°sico
- √âtica y sesgos en IA
- Aplicaciones pr√°cticas

**Nivel**: Principiante
**Duraci√≥n estimada**: 3-4 horas
**Ejercicios**: 10 ejercicios pr√°cticos

---

### üìò Notebook 2: Evoluci√≥n de los Modelos de Lenguaje
**Archivos**:
- `Evolucion_Modelos_Lenguaje.ipynb` (Parte 1)
- `Evolucion_Modelos_Lenguaje_Parte2.ipynb` (Parte 2)

Curso universitario profundo sobre la evoluci√≥n hist√≥rica y t√©cnica del NLP.

**Temas cubiertos**:

**Parte 1**:
1. Historia del NLP: de las reglas a los transformers
2. NLP Cl√°sico: tokenizaci√≥n, stemming, lemmatizaci√≥n
3. Bag of Words y TF-IDF
4. Word Embeddings: Word2Vec, GloVe
5. Sentence Transformers y b√∫squeda sem√°ntica
6. BERT: arquitectura encoder y clasificaci√≥n
7. GPT: arquitectura decoder y generaci√≥n

**Parte 2**:
8. RAG (Retrieval Augmented Generation): sistemas de pregunta-respuesta
9. Agentes de IA: planificaci√≥n y ejecuci√≥n aut√≥noma
10. Conclusiones y recursos de aprendizaje

**Nivel**: Intermedio-Avanzado
**Duraci√≥n estimada**: 6-8 horas
**Ejercicios**: 25 ejercicios pr√°cticos

---

### üéØ Notebook 3: Desaf√≠o Final
**Archivo**: `Desafio_Final_NLP.ipynb`

Proyecto integrador donde los estudiantes construyen su propio sistema completo de NLP.

**Componentes del proyecto**:
1. **NLP Cl√°sico**: Preprocesamiento y an√°lisis (15%)
2. **Embeddings**: Sistema de b√∫squeda sem√°ntica (15%)
3. **Clasificaci√≥n BERT**: Categorizaci√≥n autom√°tica (20%)
4. **Sistema RAG**: Recuperaci√≥n y generaci√≥n aumentada (20%)
5. **Generaci√≥n GPT**: Creaci√≥n de contenido inteligente (15%)
6. **Agente IA**: Orquestaci√≥n de componentes (10%)
7. **Documentaci√≥n**: An√°lisis y conclusiones (5%)

**Opciones de proyecto**:
- A) Asistente de Investigaci√≥n Acad√©mica
- B) Analizador de Noticias Inteligente
- C) Asistente de Atenci√≥n al Cliente
- D) Proyecto personalizado

**Nivel**: Proyecto integrador
**Duraci√≥n estimada**: 5-6 horas
**Ejercicios**: 19 ejercicios guiados

---

## üöÄ C√≥mo Usar Este Curso

### Opci√≥n 1: Google Colab (Recomendado)

1. **Notebook 1 - Introducci√≥n**:
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexandrarrdg/jupyter-nlp-curso/blob/main/Introduccion_a_LLM.ipynb)

2. **Notebook 2 Parte 1 - Evoluci√≥n**:
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexandrarrdg/jupyter-nlp-curso/blob/main/Evolucion_Modelos_Lenguaje.ipynb)

3. **Notebook 2 Parte 2 - Evoluci√≥n (continuaci√≥n)**:
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexandrarrdg/jupyter-nlp-curso/blob/main/Evolucion_Modelos_Lenguaje_Parte2.ipynb)

4. **Notebook 3 - Desaf√≠o Final**:
   [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexandrarrdg/jupyter-nlp-curso/blob/main/Desafio_Final_NLP.ipynb)

### Opci√≥n 2: Entorno Local

1. **Clona el repositorio**:
   ```bash
   git clone https://github.com/alexandrarrdg/jupyter-nlp-curso.git
   cd jupyter-nlp-curso
   ```

2. **Instala las dependencias**:
   ```bash
   pip install transformers sentence-transformers datasets torch scikit-learn pandas numpy matplotlib seaborn nltk spacy
   ```

3. **Abre los notebooks**:
   ```bash
   jupyter notebook
   ```

---

## üìã Requisitos

### Conocimientos Previos
- **M√≠nimo**: Programaci√≥n b√°sica en Python
- **Recomendado**: Familiaridad con NumPy y Pandas
- **No se requiere**: Experiencia previa en NLP o Machine Learning

### Requisitos T√©cnicos
- Python 3.8+
- Jupyter Notebook o Google Colab
- GPU recomendada (Colab proporciona GPU gratuita)

### Bibliotecas Principales
- `transformers` - Modelos de Hugging Face
- `sentence-transformers` - Embeddings de oraciones
- `torch` - PyTorch para deep learning
- `scikit-learn` - Machine learning cl√°sico
- `nltk` - Natural Language Toolkit
- `spacy` - NLP industrial
- `pandas`, `numpy` - An√°lisis de datos
- `matplotlib`, `seaborn` - Visualizaci√≥n

---

## üéØ Objetivos de Aprendizaje

Al completar este curso, ser√°s capaz de:

‚úÖ Entender los fundamentos del Procesamiento de Lenguaje Natural
‚úÖ Implementar pipelines completos de NLP cl√°sico
‚úÖ Utilizar modelos transformer (BERT, GPT) para tareas pr√°cticas
‚úÖ Crear sistemas de b√∫squeda sem√°ntica con embeddings
‚úÖ Desarrollar clasificadores de texto con fine-tuning
‚úÖ Construir sistemas RAG para pregunta-respuesta
‚úÖ Generar texto coherente con modelos generativos
‚úÖ Dise√±ar agentes de IA que orquestan m√∫ltiples componentes
‚úÖ Evaluar y optimizar modelos de lenguaje
‚úÖ Comprender las implicaciones √©ticas de los LLMs

---

## üìä Estructura de los Notebooks

Cada notebook sigue una estructura pedag√≥gica consistente:

1. **Celdas de Markdown**: Explicaciones te√≥ricas claras
2. **Celdas de C√≥digo**: Ejemplos ejecutables paso a paso
3. **Ejercicios Pr√°cticos**: Marcados con `# üìù EJERCICIO:`
4. **Visualizaciones**: Gr√°ficos para entender conceptos
5. **Secciones de Reflexi√≥n**: Para consolidar aprendizajes

---

## üéì Para Instructores

Este material es ideal para:

- Cursos universitarios de NLP/IA
- Bootcamps de Data Science
- Talleres corporativos de ML
- Autoaprendizaje estructurado

**Caracter√≠sticas pedag√≥gicas**:
- Progresi√≥n gradual de dificultad
- Ejemplos en espa√±ol
- Ejercicios con soluciones guiadas
- Casos de uso del mundo real
- Evaluaci√≥n con r√∫bricas claras

---

## ü§ù Contribuciones

Las contribuciones son bienvenidas. Para contribuir:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

## üìú Licencia

Este material educativo est√° disponible bajo la licencia MIT. Puedes usar, modificar y distribuir libremente con atribuci√≥n.

---

## üìß Contacto y Soporte

Para preguntas, sugerencias o reportar problemas:

- Abre un [Issue](https://github.com/alexandrarrdg/jupyter-nlp-curso/issues)
- Discusiones en [Discussions](https://github.com/alexandrarrdg/jupyter-nlp-curso/discussions)

---

## üåü Recursos Adicionales

### Documentaci√≥n Oficial
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Sentence Transformers](https://www.sbert.net/)
- [PyTorch](https://pytorch.org/docs/)

### Cursos Complementarios
- [Hugging Face Course](https://huggingface.co/course)
- [Fast.ai NLP](https://www.fast.ai/)
- [DeepLearning.AI](https://www.deeplearning.ai/)

### Papers Fundamentales
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformers
- [BERT](https://arxiv.org/abs/1810.04805) - Bidirectional Encoders
- [GPT-3](https://arxiv.org/abs/2005.14165) - Language Models

### Comunidades
- [Hugging Face Forums](https://discuss.huggingface.co/)
- [r/LanguageTechnology](https://www.reddit.com/r/LanguageTechnology/)
- [Papers with Code](https://paperswithcode.com/)

---

## üìà Roadmap del Curso

### Versi√≥n Actual: 1.0
- ‚úÖ 3 notebooks completos
- ‚úÖ 54 ejercicios pr√°cticos totales
- ‚úÖ Contenido en espa√±ol
- ‚úÖ Compatible con Google Colab

### Futuras Mejoras
- [ ] Videos explicativos complementarios
- [ ] Datasets de ejemplo pre-cargados
- [ ] Notebooks en ingl√©s
- [ ] M√≥dulo sobre LLMs locales (Llama, Mistral)
- [ ] Secci√≥n de optimizaci√≥n (LoRA, Quantization)
- [ ] Casos de estudio empresariales

---

## üôè Agradecimientos

Este curso fue desarrollado con el objetivo de democratizar el acceso al conocimiento sobre NLP y LLMs. Agradecimientos especiales a:

- La comunidad de Hugging Face por sus incre√≠bles herramientas
- Los contribuidores de bibliotecas open source
- Los estudiantes que ayudaron a mejorar el contenido

---

## ‚≠ê Si te gusta este curso

Si este material te resulta √∫til:

- Dale una ‚≠ê al repositorio
- Comp√°rtelo con otros estudiantes
- Contribuye con mejoras
- Proporciona feedback

---

**¬°Feliz aprendizaje! üöÄü§ñüìö**

---

*√öltima actualizaci√≥n: Noviembre 2025*
