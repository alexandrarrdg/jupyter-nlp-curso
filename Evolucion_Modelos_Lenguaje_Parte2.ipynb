{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìñ Evoluci√≥n de los Modelos de Lenguaje - Parte 2\n",
    "## Secciones Avanzadas: RAG, Agentes y Conclusiones\n",
    "\n",
    "**Nota**: Este notebook es continuaci√≥n de `Evolucion_Modelos_Lenguaje.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "## Contenido:\n",
    "- **Secci√≥n 8**: Mini-RAG (Retrieval Augmented Generation)\n",
    "- **Secci√≥n 9**: Mini-Agente de IA Simulado\n",
    "- **Secci√≥n 10**: Conclusiones y Recursos\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mini-RAG Introductorio\n",
    "\n",
    "### ¬øQu√© es RAG?\n",
    "\n",
    "**RAG** = **Retrieval Augmented Generation**  \n",
    "(Generaci√≥n Aumentada con Recuperaci√≥n)\n",
    "\n",
    "Es una t√©cnica que combina:\n",
    "1. **B√∫squeda** de informaci√≥n relevante en una base de conocimientos\n",
    "2. **Aumentaci√≥n** del prompt con esa informaci√≥n como contexto\n",
    "3. **Generaci√≥n** de una respuesta basada en ese contexto\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øPor qu√© es importante RAG?\n",
    "\n",
    "#### **Problema de los LLM tradicionales:**\n",
    "- ‚ùå Conocimiento limitado a datos de entrenamiento (cortado en una fecha)\n",
    "- ‚ùå No tienen acceso a informaci√≥n actualizada\n",
    "- ‚ùå No pueden consultar bases de datos privadas\n",
    "- ‚ùå **Alucinaciones**: inventan informaci√≥n que suena plausible pero es falsa\n",
    "\n",
    "#### **Soluci√≥n: RAG**\n",
    "- ‚úÖ Acceso a informaci√≥n actualizada\n",
    "- ‚úÖ Funciona con datos privados/espec√≠ficos\n",
    "- ‚úÖ Reduce alucinaciones (respuestas basadas en fuentes reales)\n",
    "- ‚úÖ Trazabilidad: puedes citar las fuentes utilizadas\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline RAG B√°sico\n",
    "\n",
    "```\n",
    "1. Usuario hace una pregunta\n",
    "   ‚Üì\n",
    "2. Generamos embedding de la pregunta\n",
    "   ‚Üì\n",
    "3. Buscamos documentos similares en la base de conocimientos (usando similitud de embeddings)\n",
    "   ‚Üì\n",
    "4. Seleccionamos los K documentos m√°s relevantes\n",
    "   ‚Üì\n",
    "5. Construimos un prompt que incluye:\n",
    "   - Contexto: documentos recuperados\n",
    "   - Pregunta del usuario\n",
    "   - Instrucci√≥n: \"Responde bas√°ndote en el contexto\"\n",
    "   ‚Üì\n",
    "6. Modelo genera respuesta usando el contexto\n",
    "   ‚Üì\n",
    "7. Devolvemos la respuesta (+ opcionalmente las fuentes)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones Reales\n",
    "\n",
    "- **Chatbots empresariales**: Acceso a documentaci√≥n interna\n",
    "- **Asistentes m√©dicos**: Consulta de literatura cient√≠fica actualizada\n",
    "- **Sistemas de soporte**: Base de conocimiento de productos\n",
    "- **Asistentes legales**: B√∫squeda en jurisprudencia\n",
    "- **Educaci√≥n**: Sistemas de tutor√≠a con materiales espec√≠ficos\n",
    "\n",
    "\n",
    "### Implementaci√≥n Pr√°ctica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librer√≠as necesarias (si no las tienes del notebook anterior)\n",
    "!pip install -q sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/imbricadas/mambaforge/envs/embeddings/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Modelos cargados\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos los modelos\n",
    "modelo_embeddings = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "generador = pipeline('text-generation', model='distilgpt2', device=-1)\n",
    "\n",
    "print(\"‚úÖ Modelos cargados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Base de Conocimiento creada\n",
      "üìä Total de documentos: 6\n",
      "\n",
      "1. Python es un lenguaje de programaci√≥n de alto nivel creado por Guido van Rossum ...\n",
      "\n",
      "2. Machine Learning es una rama de la inteligencia artificial que permite a las com...\n",
      "\n",
      "3. BERT (Bidirectional Encoder Representations from Transformers) es un modelo de l...\n",
      "\n",
      "4. GPT (Generative Pre-trained Transformer) es una familia de modelos desarrollados...\n",
      "\n",
      "5. Los transformers revolucionaron el NLP en 2017 con el paper 'Attention Is All Yo...\n",
      "\n",
      "6. RAG (Retrieval Augmented Generation) combina b√∫squeda de informaci√≥n con generac...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creamos una base de conocimiento sobre diferentes temas\n",
    "base_conocimiento = [\n",
    "    \"\"\"Python es un lenguaje de programaci√≥n de alto nivel creado por Guido van Rossum en 1991. \n",
    "    Se caracteriza por su sintaxis simple y legible, lo que lo hace ideal para principiantes. \n",
    "    Es ampliamente usado en ciencia de datos, desarrollo web, automatizaci√≥n y machine learning.\"\"\",\n",
    "    \n",
    "    \"\"\"Machine Learning es una rama de la inteligencia artificial que permite a las computadoras \n",
    "    aprender de datos sin ser expl√≠citamente programadas. Los algoritmos de ML identifican patrones \n",
    "    en los datos y hacen predicciones. Se divide en aprendizaje supervisado, no supervisado y por refuerzo.\"\"\",\n",
    "    \n",
    "    \"\"\"BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje \n",
    "    pre-entrenado desarrollado por Google en 2018. A diferencia de modelos anteriores, BERT lee \n",
    "    el texto de manera bidireccional, lo que le permite entender mejor el contexto. Es excelente \n",
    "    para tareas de comprensi√≥n como clasificaci√≥n y question answering.\"\"\",\n",
    "    \n",
    "    \"\"\"GPT (Generative Pre-trained Transformer) es una familia de modelos desarrollados por OpenAI. \n",
    "    GPT-3 tiene 175 mil millones de par√°metros y puede generar texto coherente, traducir, responder \n",
    "    preguntas y hasta programar. GPT-4 es la versi√≥n m√°s reciente y multimodal.\"\"\",\n",
    "    \n",
    "    \"\"\"Los transformers revolucionaron el NLP en 2017 con el paper 'Attention Is All You Need'. \n",
    "    Utilizan mecanismos de atenci√≥n para procesar secuencias completas en paralelo, superando \n",
    "    las limitaciones de las RNN. Son la base de modelos modernos como BERT, GPT y T5.\"\"\",\n",
    "    \n",
    "    \"\"\"RAG (Retrieval Augmented Generation) combina b√∫squeda de informaci√≥n con generaci√≥n de texto. \n",
    "    Primero busca documentos relevantes en una base de datos, luego usa esos documentos como contexto \n",
    "    para que el modelo genere una respuesta informada. Esto reduce las alucinaciones y permite \n",
    "    acceso a informaci√≥n actualizada.\"\"\"\n",
    "]\n",
    "\n",
    "print(\"üìö Base de Conocimiento creada\")\n",
    "print(f\"üìä Total de documentos: {len(base_conocimiento)}\\n\")\n",
    "\n",
    "for i, doc in enumerate(base_conocimiento, 1):\n",
    "    print(f\"{i}. {doc[:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings generados para 6 documentos\n",
      "üìä Dimensi√≥n de cada embedding: 384\n"
     ]
    }
   ],
   "source": [
    "# Generamos embeddings de todos los documentos\n",
    "embeddings_docs = modelo_embeddings.encode(base_conocimiento)\n",
    "\n",
    "print(f\"‚úÖ Embeddings generados para {len(base_conocimiento)} documentos\")\n",
    "print(f\"üìä Dimensi√≥n de cada embedding: {embeddings_docs.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sistema RAG definido\n"
     ]
    }
   ],
   "source": [
    "def buscar_documentos_relevantes(pregunta, top_k=2):\n",
    "    \"\"\"\n",
    "    Busca los documentos m√°s relevantes para una pregunta.\n",
    "    \n",
    "    Args:\n",
    "        pregunta: Pregunta del usuario\n",
    "        top_k: N√∫mero de documentos a recuperar\n",
    "    \n",
    "    Returns:\n",
    "        Lista de tuplas (documento, similitud, √≠ndice)\n",
    "    \"\"\"\n",
    "    # Generamos embedding de la pregunta\n",
    "    embedding_pregunta = modelo_embeddings.encode([pregunta])\n",
    "    \n",
    "    # Calculamos similitud con todos los documentos\n",
    "    similitudes = cosine_similarity(embedding_pregunta, embeddings_docs)[0]\n",
    "    \n",
    "    # Obtenemos los top_k m√°s similares\n",
    "    indices_top = np.argsort(similitudes)[::-1][:top_k]\n",
    "    \n",
    "    resultados = [\n",
    "        (base_conocimiento[idx], similitudes[idx], idx) \n",
    "        for idx in indices_top\n",
    "    ]\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "def sistema_rag(pregunta, usar_contexto=True, top_k=2):\n",
    "    \"\"\"\n",
    "    Sistema RAG completo: b√∫squeda + generaci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        pregunta: Pregunta del usuario\n",
    "        usar_contexto: Si False, genera sin contexto (para comparar)\n",
    "        top_k: N√∫mero de documentos a recuperar\n",
    "    \n",
    "    Returns:\n",
    "        Respuesta generada, documentos usados\n",
    "    \"\"\"\n",
    "    if usar_contexto:\n",
    "        # Paso 1: Buscar documentos relevantes\n",
    "        docs_relevantes = buscar_documentos_relevantes(pregunta, top_k=top_k)\n",
    "        \n",
    "        print(f\"\\nüîç Documentos recuperados (similitud):\")\n",
    "        for i, (doc, sim, idx) in enumerate(docs_relevantes, 1):\n",
    "            print(f\"\\n{i}. [Similitud: {sim:.3f}] Doc {idx+1}\")\n",
    "            print(f\"   {doc[:100]}...\")\n",
    "        \n",
    "        # Paso 2: Construir prompt con contexto\n",
    "        contexto = \"\\n\\n\".join([doc for doc, _, _ in docs_relevantes])\n",
    "        \n",
    "        prompt = f\"\"\"Context information:\n",
    "{contexto}\n",
    "\n",
    "Question: {pregunta}\n",
    "\n",
    "Answer based on the context above:\"\"\"\n",
    "        \n",
    "    else:\n",
    "        # Sin contexto RAG\n",
    "        prompt = f\"Question: {pregunta}\\n\\nAnswer:\"\n",
    "        docs_relevantes = []\n",
    "    \n",
    "    # Paso 3: Generar respuesta\n",
    "    respuesta = generador(\n",
    "        prompt,\n",
    "        max_length=150,\n",
    "        temperature=0.7,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )[0]['generated_text']\n",
    "    \n",
    "    return respuesta, docs_relevantes\n",
    "\n",
    "print(\"‚úÖ Sistema RAG definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "‚ùì Pregunta: What is BERT and what makes it special?\n",
      "==========================================================================================\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.519] Doc 3\n",
      "   BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje \n",
      "    pre-ent...\n",
      "\n",
      "2. [Similitud: 0.225] Doc 5\n",
      "   Los transformers revolucionaron el NLP en 2017 con el paper 'Attention Is All You Need'. \n",
      "    Utiliz...\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "üí¨ Respuesta Generada (con RAG):\n",
      "\n",
      "Context information:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje \n",
      "    pre-entrenado desarrollado por Google en 2018. A diferencia de modelos anteriores, BERT lee \n",
      "    el texto de manera bidireccional, lo que le permite entender mejor el contexto. Es excelente \n",
      "    para tareas de comprensi√≥n como clasificaci√≥n y question answering.\n",
      "\n",
      "Los transformers revolucionaron el NLP en 2017 con el paper 'Attention Is All You Need'. \n",
      "    Utilizan mecanismos de atenci√≥n para procesar secuencias completas en paralelo, superando \n",
      "    las limitaciones de las RNN. Son la base de modelos modernos como BERT, GPT y T5.\n",
      "\n",
      "Question: What is BERT and what makes it special?\n",
      "\n",
      "Answer based on the context above:¬† como ciencia a lo que le autor el rezarrollado por Google en 2018.\n",
      "La sua la pueblo de los transformers de sus empresa, a de los transformers de sus empresa.\n",
      "Le le pueblo de los transformers del empresa.\n",
      "El rezarrollado de los transformers de sus empresa.\n",
      "El rezarrollado de los transformers de sus empresa.\n",
      "El rezarrollado de los transformers de sus empresa.\n",
      "El rezarrollado de los transformers de sus empresa.\n",
      "The same article\n",
      "Google en Google en Google en Google en Google en Google en Google en 2018.\n",
      "Bidirectional Encoder Representations from Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers: Transformers:\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Probamos el sistema RAG\n",
    "pregunta_test = \"What is BERT and what makes it special?\"\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"‚ùì Pregunta: {pregunta_test}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "respuesta, docs = sistema_rag(pregunta_test, usar_contexto=True, top_k=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"\\nüí¨ Respuesta Generada (con RAG):\\n\")\n",
    "print(respuesta)\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparaci√≥n: Con RAG vs Sin RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "‚ùì Pregunta: Who created Python and when?\n",
      "==========================================================================================\n",
      "\n",
      "üü¢ CON RAG (usando contexto de la base de conocimiento):\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.648] Doc 1\n",
      "   Python es un lenguaje de programaci√≥n de alto nivel creado por Guido van Rossum en 1991. \n",
      "    Se car...\n",
      "\n",
      "2. [Similitud: 0.244] Doc 3\n",
      "   BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje \n",
      "    pre-ent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta:\n",
      "Context information:\n",
      "Python es un lenguaje de programaci√≥n de alto nivel creado por Guido van Rossum en 1991. \n",
      "    Se caracteriza por su sintaxis simple y legible, lo que lo hace ideal para principiantes. \n",
      "    Es ampliamente usado en ciencia de datos, desarrollo web, automatizaci√≥n y machine learnin...\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "üî¥ SIN RAG (sin contexto - solo conocimiento del modelo):\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "Respuesta:\n",
      "Question: Who created Python and when?\n",
      "\n",
      "Answer:\n",
      "I'm not sure. I don't know. I'm just a programmer. I'm a programmer. I'm a programmer. My job is to get me to do what I like. I'm a programmer. I'm a programmer. I'm a programmer. I'm a programmer. I'm a programmer. My job is to get me to do what I lik...\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "üí° Observa c√≥mo RAG proporciona informaci√≥n m√°s precisa basada en fuentes reales\n"
     ]
    }
   ],
   "source": [
    "# Comparamos respuestas con y sin RAG\n",
    "pregunta = \"Who created Python and when?\"\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"‚ùì Pregunta: {pregunta}\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# CON RAG\n",
    "print(\"\\nüü¢ CON RAG (usando contexto de la base de conocimiento):\")\n",
    "print(\"-\"*90)\n",
    "respuesta_rag, docs_rag = sistema_rag(pregunta, usar_contexto=True)\n",
    "print(\"\\nRespuesta:\")\n",
    "print(respuesta_rag[:300] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "\n",
    "# SIN RAG\n",
    "print(\"\\nüî¥ SIN RAG (sin contexto - solo conocimiento del modelo):\")\n",
    "print(\"-\"*90)\n",
    "respuesta_sin_rag, _ = sistema_rag(pregunta, usar_contexto=False)\n",
    "print(\"\\nRespuesta:\")\n",
    "print(respuesta_sin_rag[:300] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"\\nüí° Observa c√≥mo RAG proporciona informaci√≥n m√°s precisa basada en fuentes reales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù EJERCICIO 20: Cambia la base de conocimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Completa 'mi_base_conocimiento' y 'mis_preguntas'\n",
      "\n",
      "üí° Sugerencias de temas:\n",
      "   - Tu √°rea de estudio o investigaci√≥n\n",
      "   - Hobby o inter√©s personal (m√∫sica, deportes, cocina)\n",
      "   - Temas de actualidad que conozcas bien\n"
     ]
    }
   ],
   "source": [
    "# üìù EJERCICIO: Crea tu propia base de conocimiento sobre un tema de tu inter√©s\n",
    "\n",
    "# Define tu base de conocimiento aqu√≠:\n",
    "mi_base_conocimiento = [\n",
    "    # A√±ade al menos 4-5 p√°rrafos sobre un tema espec√≠fico\n",
    "    # Ejemplo tema: Historia de la computaci√≥n, Cambio clim√°tico, Nutrici√≥n, etc.\n",
    "    \n",
    "]\n",
    "\n",
    "# Define preguntas relacionadas:\n",
    "mis_preguntas = [\n",
    "    # A√±ade 3-5 preguntas que puedan responderse con tu base de conocimiento\n",
    "    \n",
    "]\n",
    "\n",
    "if mi_base_conocimiento and mis_preguntas:\n",
    "    # Generamos embeddings\n",
    "    mis_embeddings = modelo_embeddings.encode(mi_base_conocimiento)\n",
    "    \n",
    "    # Funci√≥n RAG adaptada\n",
    "    def mi_rag(pregunta):\n",
    "        emb_pregunta = modelo_embeddings.encode([pregunta])\n",
    "        similitudes = cosine_similarity(emb_pregunta, mis_embeddings)[0]\n",
    "        idx_mejor = np.argmax(similitudes)\n",
    "        \n",
    "        contexto = mi_base_conocimiento[idx_mejor]\n",
    "        prompt = f\"Context: {contexto}\\n\\nQuestion: {pregunta}\\n\\nAnswer:\"\n",
    "        \n",
    "        respuesta = generador(\n",
    "            prompt, max_length=120, temperature=0.7, do_sample=True, pad_token_id=50256\n",
    "        )[0]['generated_text']\n",
    "        \n",
    "        return respuesta, contexto, similitudes[idx_mejor]\n",
    "    \n",
    "    # Probamos con tus preguntas\n",
    "    for i, pregunta in enumerate(mis_preguntas, 1):\n",
    "        print(f\"\\n{'='*90}\")\n",
    "        print(f\"Pregunta {i}: {pregunta}\")\n",
    "        print(f\"{'='*90}\")\n",
    "        \n",
    "        respuesta, ctx, sim = mi_rag(pregunta)\n",
    "        \n",
    "        print(f\"\\nüîç Contexto usado (similitud: {sim:.3f}):\")\n",
    "        print(f\"   {ctx[:150]}...\")\n",
    "        print(f\"\\nüí¨ Respuesta:\\n{respuesta}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Completa 'mi_base_conocimiento' y 'mis_preguntas'\")\n",
    "    print(\"\\nüí° Sugerencias de temas:\")\n",
    "    print(\"   - Tu √°rea de estudio o investigaci√≥n\")\n",
    "    print(\"   - Hobby o inter√©s personal (m√∫sica, deportes, cocina)\")\n",
    "    print(\"   - Temas de actualidad que conozcas bien\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EJERCICIO 21: Comparaci√≥n con y sin contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä An√°lisis Comparativo: RAG vs Sin RAG\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "\n",
      "##########################################################################################\n",
      "PREGUNTA 1: What are transformers in NLP?\n",
      "##########################################################################################\n",
      "\n",
      "‚úÖ CON RAG:\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.661] Doc 5\n",
      "   Los transformers revolucionaron el NLP en 2017 con el paper 'Attention Is All You Need'. \n",
      "    Utiliz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta: Context information:\n",
      "Los transformers revolucionaron el NLP en 2017 con el paper 'Attention Is All You Need'. \n",
      "    Utilizan mecanismos de atenci√≥n para procesar secuencias completas en paralelo, superando \n",
      "    las limitaciones de las RNN. Son la base...\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "‚ùå SIN RAG:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta: Question: What are transformers in NLP?\n",
      "\n",
      "Answer: These are two things that are most often discussed in the book:\n",
      "The NLP is a system where we can see the state of a system that is in the current state of a system that is in the current state of a sys...\n",
      "\n",
      "\n",
      "##########################################################################################\n",
      "PREGUNTA 2: Explain the difference between BERT and GPT\n",
      "##########################################################################################\n",
      "\n",
      "‚úÖ CON RAG:\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.631] Doc 4\n",
      "   GPT (Generative Pre-trained Transformer) es una familia de modelos desarrollados por OpenAI. \n",
      "    GP...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta: Context information:\n",
      "GPT (Generative Pre-trained Transformer) es una familia de modelos desarrollados por OpenAI. \n",
      "    GPT-3 tiene 175 mil millones de par√°metros y puede generar texto coherente, traducir, responder \n",
      "    preguntas y hasta programar. G...\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "‚ùå SIN RAG:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta: Question: Explain the difference between BERT and GPT\n",
      "\n",
      "Answer: This question is not to be confused with BERT\n",
      "Answer: The difference between BERT and GPT\n",
      "Answer: The difference between BERT and GPT\n",
      "Answer: The difference between BERT and GPT\n",
      "Answer: T...\n",
      "\n",
      "\n",
      "##########################################################################################\n",
      "PREGUNTA 3: What is RAG and why is it useful?\n",
      "##########################################################################################\n",
      "\n",
      "‚úÖ CON RAG:\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.610] Doc 6\n",
      "   RAG (Retrieval Augmented Generation) combina b√∫squeda de informaci√≥n con generaci√≥n de texto. \n",
      "    P...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Respuesta: Context information:\n",
      "RAG (Retrieval Augmented Generation) combina b√∫squeda de informaci√≥n con generaci√≥n de texto. \n",
      "    Primero busca documentos relevantes en una base de datos, luego usa esos documentos como contexto \n",
      "    para que el modelo genere u...\n",
      "\n",
      "------------------------------------------------------------------------------------------\n",
      "\n",
      "‚ùå SIN RAG:\n",
      "\n",
      "Respuesta: Question: What is RAG and why is it useful?\n",
      "\n",
      "Answer: The RAG's purpose is to help people understand the importance of RAG and why it should be used.\n",
      "Question: Which RAGs help people understand?\n",
      "Answer: RAG is a set of six different domains that help ...\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "üí° Reflexi√≥n:\n",
      "   ¬øCu√°l versi√≥n proporciona informaci√≥n m√°s precisa?\n",
      "   ¬øEl modelo sin RAG 'alucina' o inventa informaci√≥n?\n",
      "   ¬øC√≥mo se podr√≠a mejorar el sistema RAG?\n"
     ]
    }
   ],
   "source": [
    "# üìù EJERCICIO: Analiza c√≥mo cambia la calidad con/sin RAG\n",
    "\n",
    "preguntas_comparacion = [\n",
    "    \"What are transformers in NLP?\",\n",
    "    \"Explain the difference between BERT and GPT\",\n",
    "    \"What is RAG and why is it useful?\"\n",
    "]\n",
    "\n",
    "print(\"üìä An√°lisis Comparativo: RAG vs Sin RAG\\n\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "for i, pregunta in enumerate(preguntas_comparacion, 1):\n",
    "    print(f\"\\n\\n{'#'*90}\")\n",
    "    print(f\"PREGUNTA {i}: {pregunta}\")\n",
    "    print(f\"{'#'*90}\")\n",
    "    \n",
    "    # Con RAG\n",
    "    print(\"\\n‚úÖ CON RAG:\")\n",
    "    resp_rag, docs = sistema_rag(pregunta, usar_contexto=True, top_k=1)\n",
    "    print(f\"\\nRespuesta: {resp_rag[:250]}...\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*90)\n",
    "    \n",
    "    # Sin RAG\n",
    "    print(\"\\n‚ùå SIN RAG:\")\n",
    "    resp_sin, _ = sistema_rag(pregunta, usar_contexto=False)\n",
    "    print(f\"\\nRespuesta: {resp_sin[:250]}...\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*90)\n",
    "print(\"\\nüí° Reflexi√≥n:\")\n",
    "print(\"   ¬øCu√°l versi√≥n proporciona informaci√≥n m√°s precisa?\")\n",
    "print(\"   ¬øEl modelo sin RAG 'alucina' o inventa informaci√≥n?\")\n",
    "print(\"   ¬øC√≥mo se podr√≠a mejorar el sistema RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EJERCICIO 22: An√°lisis de errores del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç An√°lisis de Errores: Preguntas fuera del dominio\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "‚ö†Ô∏è Estas preguntas NO est√°n cubiertas en nuestra base de conocimiento\n",
      "   Observa qu√© hace el sistema RAG en estos casos\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "Pregunta 1: What is the capital of France?\n",
      "==========================================================================================\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.005] Doc 1\n",
      "   Python es un lenguaje de programaci√≥n de alto nivel creado por Guido van Rossum en 1991. \n",
      "    Se car...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Respuesta generada:\n",
      "Context information:\n",
      "Python es un lenguaje de programaci√≥n de alto nivel creado por Guido van Rossum en 1991. \n",
      "    Se caracteriza por su sintaxis simple y legible, lo que lo hace ideal para principian...\n",
      "\n",
      "‚ùì ¬øEl contexto recuperado es relevante para la pregunta?\n",
      "   Similitud del mejor documento: 0.005\n",
      "   ‚ö†Ô∏è ALERTA: Similitud muy baja - probablemente respuesta no confiable\n",
      "\n",
      "==========================================================================================\n",
      "Pregunta 2: How do you cook pasta?\n",
      "==========================================================================================\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.109] Doc 3\n",
      "   BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje \n",
      "    pre-ent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Respuesta generada:\n",
      "Context information:\n",
      "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de lenguaje \n",
      "    pre-entrenado desarrollado por Google en 2018. A diferencia de modelos anteriores, BER...\n",
      "\n",
      "‚ùì ¬øEl contexto recuperado es relevante para la pregunta?\n",
      "   Similitud del mejor documento: 0.109\n",
      "   ‚ö†Ô∏è ALERTA: Similitud muy baja - probablemente respuesta no confiable\n",
      "\n",
      "==========================================================================================\n",
      "Pregunta 3: Who won the World Cup in 2022?\n",
      "==========================================================================================\n",
      "\n",
      "üîç Documentos recuperados (similitud):\n",
      "\n",
      "1. [Similitud: 0.108] Doc 4\n",
      "   GPT (Generative Pre-trained Transformer) es una familia de modelos desarrollados por OpenAI. \n",
      "    GP...\n",
      "\n",
      "üí¨ Respuesta generada:\n",
      "Context information:\n",
      "GPT (Generative Pre-trained Transformer) es una familia de modelos desarrollados por OpenAI. \n",
      "    GPT-3 tiene 175 mil millones de par√°metros y puede generar texto coherente, tradu...\n",
      "\n",
      "‚ùì ¬øEl contexto recuperado es relevante para la pregunta?\n",
      "   Similitud del mejor documento: 0.108\n",
      "   ‚ö†Ô∏è ALERTA: Similitud muy baja - probablemente respuesta no confiable\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "üí° Mejoras posibles:\n",
      "   1. Definir umbral de similitud m√≠nimo\n",
      "   2. Si similitud < umbral ‚Üí Responder 'No tengo informaci√≥n suficiente'\n",
      "   3. Ampliar la base de conocimiento\n",
      "   4. Implementar re-ranking de documentos\n"
     ]
    }
   ],
   "source": [
    "# üìù EJERCICIO: Identifica cu√°ndo el modelo comete errores\n",
    "\n",
    "# Preguntas que NO est√°n en la base de conocimiento\n",
    "preguntas_fuera_dominio = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"How do you cook pasta?\",\n",
    "    \"Who won the World Cup in 2022?\"\n",
    "]\n",
    "\n",
    "print(\"üîç An√°lisis de Errores: Preguntas fuera del dominio\\n\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\n‚ö†Ô∏è Estas preguntas NO est√°n cubiertas en nuestra base de conocimiento\")\n",
    "print(\"   Observa qu√© hace el sistema RAG en estos casos\\n\")\n",
    "\n",
    "for i, pregunta in enumerate(preguntas_fuera_dominio, 1):\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"Pregunta {i}: {pregunta}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    \n",
    "    respuesta, docs = sistema_rag(pregunta, usar_contexto=True, top_k=1)\n",
    "    \n",
    "    print(f\"\\nüí¨ Respuesta generada:\\n{respuesta[:200]}...\")\n",
    "    \n",
    "    print(f\"\\n‚ùì ¬øEl contexto recuperado es relevante para la pregunta?\")\n",
    "    if docs:\n",
    "        sim = docs[0][1]\n",
    "        print(f\"   Similitud del mejor documento: {sim:.3f}\")\n",
    "        if sim < 0.3:\n",
    "            print(\"   ‚ö†Ô∏è ALERTA: Similitud muy baja - probablemente respuesta no confiable\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*90)\n",
    "print(\"\\nüí° Mejoras posibles:\")\n",
    "print(\"   1. Definir umbral de similitud m√≠nimo\")\n",
    "print(\"   2. Si similitud < umbral ‚Üí Responder 'No tengo informaci√≥n suficiente'\")\n",
    "print(\"   3. Ampliar la base de conocimiento\")\n",
    "print(\"   4. Implementar re-ranking de documentos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Mini-Agente de IA Simulado\n",
    "\n",
    "### ¬øQu√© es un Agente de IA?\n",
    "\n",
    "Un **agente de IA** es un sistema aut√≥nomo capaz de:\n",
    "\n",
    "1. **Percibir**: Recibir un objetivo o tarea\n",
    "2. **Planificar**: Descomponer la tarea en pasos\n",
    "3. **Actuar**: Ejecutar acciones usando herramientas\n",
    "4. **Aprender**: Ajustar seg√∫n resultados\n",
    "\n",
    "---\n",
    "\n",
    "### Componentes de un Agente\n",
    "\n",
    "#### **1. Objetivo (Goal)**\n",
    "```\n",
    "\"Organizar un viaje a Par√≠s para 3 personas en julio\"\n",
    "```\n",
    "\n",
    "#### **2. Planificaci√≥n**\n",
    "```\n",
    "1. Buscar vuelos disponibles\n",
    "2. Comparar precios de hoteles\n",
    "3. Crear itinerario de actividades\n",
    "4. Calcular presupuesto total\n",
    "```\n",
    "\n",
    "#### **3. Herramientas/Acciones**\n",
    "```python\n",
    "- buscar_vuelos(origen, destino, fecha)\n",
    "- buscar_hoteles(ciudad, fechas, personas)\n",
    "- buscar_actividades(ciudad)\n",
    "- calcular_costos(lista_items)\n",
    "```\n",
    "\n",
    "#### **4. Ejecuci√≥n**\n",
    "El agente ejecuta cada paso del plan usando las herramientas disponibles\n",
    "\n",
    "---\n",
    "\n",
    "### Tipos de Agentes\n",
    "\n",
    "| Tipo | Descripci√≥n | Ejemplo |\n",
    "|------|-------------|----------|\n",
    "| **Reactivo** | Responde a est√≠mulos inmediatos | Chatbot simple |\n",
    "| **Planificador** | Crea plan antes de actuar | Asistente de viajes |\n",
    "| **Aprendizaje** | Mejora con experiencia | Agente de RL |\n",
    "| **Multi-agente** | Colabora con otros agentes | Sistema distribuido |\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones Reales\n",
    "\n",
    "- **AutoGPT**: Agente que se auto-asigna sub-tareas\n",
    "- **BabyAGI**: Agente con gesti√≥n de tareas\n",
    "- **LangChain Agents**: Framework para construir agentes\n",
    "- **ChatGPT con plugins**: Agente con acceso a herramientas\n",
    "\n",
    "---\n",
    "\n",
    "### Implementaci√≥n Simple\n",
    "\n",
    "Vamos a crear un agente MUY b√°sico que:\n",
    "1. Recibe un objetivo\n",
    "2. Usa el modelo generativo para crear un plan\n",
    "3. Simula la ejecuci√≥n de cada paso\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Herramientas del agente definidas\n"
     ]
    }
   ],
   "source": [
    "# Definimos funciones \"dummy\" que simulan herramientas del agente\n",
    "\n",
    "def buscar_informacion(tema):\n",
    "    \"\"\"Simula b√∫squeda en internet\"\"\"\n",
    "    print(f\"  üîç [HERRAMIENTA] Buscando informaci√≥n sobre: '{tema}'\")\n",
    "    print(f\"     ‚úì B√∫squeda completada (simulado)\")\n",
    "    return f\"Informaci√≥n encontrada sobre {tema}\"\n",
    "\n",
    "def leer_archivo(nombre_archivo):\n",
    "    \"\"\"Simula lectura de archivo\"\"\"\n",
    "    print(f\"  üìÑ [HERRAMIENTA] Leyendo archivo: '{nombre_archivo}'\")\n",
    "    print(f\"     ‚úì Archivo le√≠do (simulado)\")\n",
    "    return f\"Contenido de {nombre_archivo}\"\n",
    "\n",
    "def guardar_resultado(datos, archivo):\n",
    "    \"\"\"Simula guardar datos en archivo\"\"\"\n",
    "    print(f\"  üíæ [HERRAMIENTA] Guardando datos en: '{archivo}'\")\n",
    "    print(f\"     ‚úì Datos guardados (simulado)\")\n",
    "    return f\"Datos guardados en {archivo}\"\n",
    "\n",
    "def enviar_notificacion(mensaje):\n",
    "    \"\"\"Simula env√≠o de notificaci√≥n\"\"\"\n",
    "    print(f\"  üìß [HERRAMIENTA] Enviando notificaci√≥n: '{mensaje[:50]}...'\")\n",
    "    print(f\"     ‚úì Notificaci√≥n enviada (simulado)\")\n",
    "    return \"Notificaci√≥n enviada\"\n",
    "\n",
    "def analizar_datos(datos):\n",
    "    \"\"\"Simula an√°lisis de datos\"\"\"\n",
    "    print(f\"  üìä [HERRAMIENTA] Analizando datos...\")\n",
    "    print(f\"     ‚úì An√°lisis completado (simulado)\")\n",
    "    return \"Resultados del an√°lisis\"\n",
    "\n",
    "print(\"‚úÖ Herramientas del agente definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sistema de agente definido\n"
     ]
    }
   ],
   "source": [
    "def planificar_objetivo(objetivo):\n",
    "    \"\"\"\n",
    "    Usa el modelo generativo para crear un plan de acci√≥n.\n",
    "    \n",
    "    Args:\n",
    "        objetivo: Descripci√≥n del objetivo a lograr\n",
    "    \n",
    "    Returns:\n",
    "        Lista de pasos del plan\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Create a step-by-step action plan to achieve this goal: {objetivo}\n",
    "\n",
    "Format the plan as numbered steps:\n",
    "1. [First action]\n",
    "2. [Second action]\n",
    "3. [Third action]\n",
    "4. [Fourth action]\n",
    "\n",
    "Keep it practical and focused (maximum 5 steps).\n",
    "\n",
    "Action Plan:\"\"\"\n",
    "    \n",
    "    plan_texto = generador(\n",
    "        prompt,\n",
    "        max_length=150,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )[0]['generated_text']\n",
    "    \n",
    "    # Extraemos los pasos del plan\n",
    "    pasos = []\n",
    "    for linea in plan_texto.split('\\n'):\n",
    "        linea = linea.strip()\n",
    "        # Buscamos l√≠neas que empiecen con n√∫mero\n",
    "        if linea and (linea[0].isdigit() or linea.startswith('-')):\n",
    "            # Limpiamos el paso\n",
    "            paso = linea.lstrip('0123456789.-) ').strip()\n",
    "            if paso and len(paso) > 10:  # Filtramos pasos muy cortos\n",
    "                pasos.append(paso)\n",
    "    \n",
    "    # Si no encontramos pasos estructurados, devolvemos el texto completo dividido\n",
    "    if not pasos:\n",
    "        pasos = [plan_texto]\n",
    "    \n",
    "    return pasos[:5]  # M√°ximo 5 pasos\n",
    "\n",
    "def ejecutar_paso(paso, num_paso):\n",
    "    \"\"\"\n",
    "    Ejecuta un paso del plan llamando a la herramienta apropiada.\n",
    "    \n",
    "    Args:\n",
    "        paso: Descripci√≥n del paso\n",
    "        num_paso: N√∫mero del paso\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìå Ejecutando Paso {num_paso}: {paso}\")\n",
    "    print(f\"{'‚îÄ'*85}\")\n",
    "    \n",
    "    paso_lower = paso.lower()\n",
    "    \n",
    "    # L√≥gica simple para decidir qu√© herramienta usar\n",
    "    if any(palabra in paso_lower for palabra in ['search', 'find', 'look', 'research']):\n",
    "        buscar_informacion(paso)\n",
    "    elif any(palabra in paso_lower for palabra in ['read', 'review', 'check', 'analyze']):\n",
    "        if 'data' in paso_lower or 'analyze' in paso_lower:\n",
    "            analizar_datos(paso)\n",
    "        else:\n",
    "            leer_archivo(\"documento.txt\")\n",
    "    elif any(palabra in paso_lower for palabra in ['save', 'write', 'create', 'generate']):\n",
    "        guardar_resultado(paso, \"resultado.txt\")\n",
    "    elif any(palabra in paso_lower for palabra in ['send', 'notify', 'email', 'message']):\n",
    "        enviar_notificacion(paso)\n",
    "    else:\n",
    "        print(f\"  ‚öôÔ∏è [ACCI√ìN GEN√âRICA] Ejecutando: {paso[:60]}...\")\n",
    "        print(f\"     ‚úì Completado\")\n",
    "\n",
    "def agente_autonomo(objetivo):\n",
    "    \"\"\"\n",
    "    Agente completo: planifica y ejecuta de forma aut√≥noma.\n",
    "    \n",
    "    Args:\n",
    "        objetivo: Objetivo a lograr\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"ü§ñ AGENTE DE IA AUT√ìNOMO\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"\\nüéØ OBJETIVO: {objetivo}\")\n",
    "    print(\"\\n\" + \"‚îÄ\"*90)\n",
    "    \n",
    "    # Fase 1: Planificaci√≥n\n",
    "    print(\"\\nüß† FASE 1: PLANIFICACI√ìN\")\n",
    "    print(\"‚îÄ\"*90)\n",
    "    print(\"\\nGenerando plan de acci√≥n...\\n\")\n",
    "    \n",
    "    pasos = planificar_objetivo(objetivo)\n",
    "    \n",
    "    print(\"üìã PLAN GENERADO:\")\n",
    "    for i, paso in enumerate(pasos, 1):\n",
    "        print(f\"   {i}. {paso}\")\n",
    "    \n",
    "    # Fase 2: Ejecuci√≥n\n",
    "    print(\"\\n\" + \"‚îÄ\"*90)\n",
    "    print(\"\\n‚öôÔ∏è FASE 2: EJECUCI√ìN\")\n",
    "    print(\"‚îÄ\"*90)\n",
    "    \n",
    "    for i, paso in enumerate(pasos, 1):\n",
    "        ejecutar_paso(paso, i)\n",
    "    \n",
    "    # Finalizaci√≥n\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"‚úÖ AGENTE COMPLETADO - Objetivo alcanzado\")\n",
    "    print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Sistema de agente definido\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "ü§ñ AGENTE DE IA AUT√ìNOMO\n",
      "==========================================================================================\n",
      "\n",
      "üéØ OBJETIVO: Research and summarize the latest trends in artificial intelligence\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üß† FASE 1: PLANIFICACI√ìN\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "Generando plan de acci√≥n...\n",
      "\n",
      "üìã PLAN GENERADO:\n",
      "   1. [First action]\n",
      "   2. [Second action]\n",
      "   3. [Third action]\n",
      "   4. [Fourth action]\n",
      "   5. [Second action]\n",
      "\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "‚öôÔ∏è FASE 2: EJECUCI√ìN\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "\n",
      "üìå Ejecutando Paso 1: [First action]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  ‚öôÔ∏è [ACCI√ìN GEN√âRICA] Ejecutando: [First action]...\n",
      "     ‚úì Completado\n",
      "\n",
      "üìå Ejecutando Paso 2: [Second action]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  ‚öôÔ∏è [ACCI√ìN GEN√âRICA] Ejecutando: [Second action]...\n",
      "     ‚úì Completado\n",
      "\n",
      "üìå Ejecutando Paso 3: [Third action]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  ‚öôÔ∏è [ACCI√ìN GEN√âRICA] Ejecutando: [Third action]...\n",
      "     ‚úì Completado\n",
      "\n",
      "üìå Ejecutando Paso 4: [Fourth action]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  ‚öôÔ∏è [ACCI√ìN GEN√âRICA] Ejecutando: [Fourth action]...\n",
      "     ‚úì Completado\n",
      "\n",
      "üìå Ejecutando Paso 5: [Second action]\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "  ‚öôÔ∏è [ACCI√ìN GEN√âRICA] Ejecutando: [Second action]...\n",
      "     ‚úì Completado\n",
      "\n",
      "==========================================================================================\n",
      "‚úÖ AGENTE COMPLETADO - Objetivo alcanzado\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Probamos el agente con un objetivo\n",
    "objetivo_ejemplo = \"Research and summarize the latest trends in artificial intelligence\"\n",
    "\n",
    "agente_autonomo(objetivo_ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìù EJERCICIO 23: Cambia el objetivo del agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Define un objetivo en la variable 'mi_objetivo'\n",
      "\n",
      "üí° Ejemplos de objetivos:\n",
      "   - 'Organize a team-building event for 20 people'\n",
      "   - 'Write and publish a blog post about climate change'\n",
      "   - 'Prepare a budget analysis for next quarter'\n",
      "   - 'Learn to play guitar basics in one month'\n"
     ]
    }
   ],
   "source": [
    "# üìù EJERCICIO: Define tu propio objetivo y observa c√≥mo el agente lo descompone\n",
    "\n",
    "# Define tu objetivo aqu√≠:\n",
    "mi_objetivo = \"\"  # Ejemplos:\n",
    "                   # \"Create a marketing campaign for a new product\"\n",
    "                   # \"Plan a healthy meal prep for the week\"\n",
    "                   # \"Learn Python programming in 30 days\"\n",
    "\n",
    "if mi_objetivo:\n",
    "    agente_autonomo(mi_objetivo)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Define un objetivo en la variable 'mi_objetivo'\")\n",
    "    print(\"\\nüí° Ejemplos de objetivos:\")\n",
    "    print(\"   - 'Organize a team-building event for 20 people'\")\n",
    "    print(\"   - 'Write and publish a blog post about climate change'\")\n",
    "    print(\"   - 'Prepare a budget analysis for next quarter'\")\n",
    "    print(\"   - 'Learn to play guitar basics in one month'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EJERCICIO 24: A√±ade una nueva herramienta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agente mejorado definido\n",
      "\n",
      "üí° Completa:\n",
      "   1. La funci√≥n 'mi_herramienta_personalizada'\n",
      "   2. La condici√≥n en 'ejecutar_paso_mejorado'\n",
      "   3. Prueba con un objetivo que active tu herramienta\n"
     ]
    }
   ],
   "source": [
    "# üìù EJERCICIO: Crea tu propia herramienta y a√±√°dela al agente\n",
    "\n",
    "def mi_herramienta_personalizada(parametro):\n",
    "    \"\"\"\n",
    "    Define aqu√≠ tu herramienta personalizada.\n",
    "    \n",
    "    Ejemplos:\n",
    "    - calcular_costos()\n",
    "    - traducir_texto()\n",
    "    - generar_reporte()\n",
    "    - enviar_recordatorio()\n",
    "    \"\"\"\n",
    "    print(f\"  üîß [MI HERRAMIENTA] Ejecutando acci√≥n personalizada...\")\n",
    "    print(f\"     Par√°metro: {parametro[:50]}...\")\n",
    "    print(f\"     ‚úì Acci√≥n completada\")\n",
    "    return \"Resultado de mi herramienta\"\n",
    "\n",
    "def ejecutar_paso_mejorado(paso, num_paso):\n",
    "    \"\"\"Versi√≥n mejorada con tu herramienta incluida\"\"\"\n",
    "    print(f\"\\nüìå Ejecutando Paso {num_paso}: {paso}\")\n",
    "    print(f\"{'‚îÄ'*85}\")\n",
    "    \n",
    "    paso_lower = paso.lower()\n",
    "    \n",
    "    # A√ëADE AQU√ç LA CONDICI√ìN PARA TU HERRAMIENTA\n",
    "    # Ejemplo:\n",
    "    # if 'palabra_clave' in paso_lower:\n",
    "    #     mi_herramienta_personalizada(paso)\n",
    "    \n",
    "    # Herramientas existentes\n",
    "    if any(palabra in paso_lower for palabra in ['search', 'find', 'research']):\n",
    "        buscar_informacion(paso)\n",
    "    elif any(palabra in paso_lower for palabra in ['read', 'review', 'check']):\n",
    "        leer_archivo(\"documento.txt\")\n",
    "    elif any(palabra in paso_lower for palabra in ['save', 'write', 'create']):\n",
    "        guardar_resultado(paso, \"resultado.txt\")\n",
    "    elif any(palabra in paso_lower for palabra in ['send', 'notify']):\n",
    "        enviar_notificacion(paso)\n",
    "    # A√ëADE TU CONDICI√ìN AQU√ç\n",
    "    \n",
    "    else:\n",
    "        print(f\"  ‚öôÔ∏è [ACCI√ìN GEN√âRICA] {paso[:60]}...\")\n",
    "        print(f\"     ‚úì Completado\")\n",
    "\n",
    "def agente_mejorado(objetivo):\n",
    "    \"\"\"Agente que usa tu herramienta personalizada\"\"\"\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"ü§ñ AGENTE MEJORADO (con herramienta personalizada)\")\n",
    "    print(\"=\"*90)\n",
    "    print(f\"\\nüéØ OBJETIVO: {objetivo}\\n\")\n",
    "    \n",
    "    pasos = planificar_objetivo(objetivo)\n",
    "    \n",
    "    print(\"üìã PLAN:\")\n",
    "    for i, paso in enumerate(pasos, 1):\n",
    "        print(f\"   {i}. {paso}\")\n",
    "    \n",
    "    print(\"\\n\" + \"‚îÄ\"*90)\n",
    "    print(\"\\n‚öôÔ∏è EJECUTANDO:\\n\")\n",
    "    \n",
    "    for i, paso in enumerate(pasos, 1):\n",
    "        ejecutar_paso_mejorado(paso, i)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"‚úÖ COMPLETADO\")\n",
    "    print(\"=\"*90 + \"\\n\")\n",
    "\n",
    "# Prueba tu agente mejorado:\n",
    "# agente_mejorado(\"Tu objetivo aqu√≠\")\n",
    "\n",
    "print(\"‚úÖ Agente mejorado definido\")\n",
    "print(\"\\nüí° Completa:\")\n",
    "print(\"   1. La funci√≥n 'mi_herramienta_personalizada'\")\n",
    "print(\"   2. La condici√≥n en 'ejecutar_paso_mejorado'\")\n",
    "print(\"   3. Prueba con un objetivo que active tu herramienta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù EJERCICIO 25: Identifica decisiones incorrectas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç An√°lisis de Decisiones del Agente\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "Probando con objetivos POCO REALISTAS o DEMASIADO COMPLEJOS\n",
      "Observa si el agente:\n",
      "  - Genera planes realistas\n",
      "  - Identifica limitaciones\n",
      "  - Divide apropiadamente tareas complejas\n",
      "\n",
      "\n",
      "##########################################################################################\n",
      "CASO 1: Build a house from scratch\n",
      "##########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Plan generado:\n",
      "   1. [First action]\n",
      "   2. [Second action]\n",
      "   3. [Third action]\n",
      "   4. [Fourth action]\n",
      "   5. [Second action]\n",
      "\n",
      "ü§î An√°lisis:\n",
      "   ¬øEl plan es realista?\n",
      "   ¬øFalta alg√∫n paso importante?\n",
      "   ¬øLos pasos son demasiado vagos o espec√≠ficos?\n",
      "\n",
      "##########################################################################################\n",
      "CASO 2: Become a professional athlete in one week\n",
      "##########################################################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=150) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã Plan generado:\n",
      "   1. [First action]\n",
      "   2. [Second action]\n",
      "   3. [Third action]\n",
      "   4. [Fourth action]\n",
      "\n",
      "ü§î An√°lisis:\n",
      "   ¬øEl plan es realista?\n",
      "   ¬øFalta alg√∫n paso importante?\n",
      "   ¬øLos pasos son demasiado vagos o espec√≠ficos?\n",
      "\n",
      "##########################################################################################\n",
      "CASO 3: Solve world hunger\n",
      "##########################################################################################\n",
      "\n",
      "üìã Plan generado:\n",
      "   1. [First action]\n",
      "   2. [Second action]\n",
      "   3. [Third action]\n",
      "   4. [Fourth action]\n",
      "   5. [Second action]\n",
      "\n",
      "ü§î An√°lisis:\n",
      "   ¬øEl plan es realista?\n",
      "   ¬øFalta alg√∫n paso importante?\n",
      "   ¬øLos pasos son demasiado vagos o espec√≠ficos?\n",
      "\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "üí° Limitaciones observadas:\n",
      "   - El modelo es peque√±o (DistilGPT-2) ‚Üí planes simples\n",
      "   - No tiene sentido com√∫n real ‚Üí puede ser poco realista\n",
      "   - No verifica viabilidad ‚Üí acepta cualquier objetivo\n",
      "   - No tiene memoria ‚Üí cada plan es independiente\n",
      "\n",
      "üöÄ Mejoras posibles:\n",
      "   - Usar modelos m√°s grandes (GPT-4, Claude)\n",
      "   - A√±adir validaci√≥n de viabilidad\n",
      "   - Implementar memoria de acciones previas\n",
      "   - Permitir replanning si una acci√≥n falla\n"
     ]
    }
   ],
   "source": [
    "# üìù EJERCICIO: Analiza cu√°ndo el agente toma decisiones sub√≥ptimas\n",
    "\n",
    "objetivos_desafiantes = [\n",
    "    \"Build a house from scratch\",\n",
    "    \"Become a professional athlete in one week\",\n",
    "    \"Solve world hunger\"\n",
    "]\n",
    "\n",
    "print(\"üîç An√°lisis de Decisiones del Agente\\n\")\n",
    "print(\"=\"*90)\n",
    "print(\"\\nProbando con objetivos POCO REALISTAS o DEMASIADO COMPLEJOS\")\n",
    "print(\"Observa si el agente:\")\n",
    "print(\"  - Genera planes realistas\")\n",
    "print(\"  - Identifica limitaciones\")\n",
    "print(\"  - Divide apropiadamente tareas complejas\\n\")\n",
    "\n",
    "for i, objetivo in enumerate(objetivos_desafiantes, 1):\n",
    "    print(f\"\\n{'#'*90}\")\n",
    "    print(f\"CASO {i}: {objetivo}\")\n",
    "    print(f\"{'#'*90}\")\n",
    "    \n",
    "    pasos = planificar_objetivo(objetivo)\n",
    "    \n",
    "    print(\"\\nüìã Plan generado:\")\n",
    "    for j, paso in enumerate(pasos, 1):\n",
    "        print(f\"   {j}. {paso}\")\n",
    "    \n",
    "    print(\"\\nü§î An√°lisis:\")\n",
    "    print(\"   ¬øEl plan es realista?\")\n",
    "    print(\"   ¬øFalta alg√∫n paso importante?\")\n",
    "    print(\"   ¬øLos pasos son demasiado vagos o espec√≠ficos?\")\n",
    "\n",
    "print(\"\\n\\n\" + \"=\"*90)\n",
    "print(\"\\nüí° Limitaciones observadas:\")\n",
    "print(\"   - El modelo es peque√±o (DistilGPT-2) ‚Üí planes simples\")\n",
    "print(\"   - No tiene sentido com√∫n real ‚Üí puede ser poco realista\")\n",
    "print(\"   - No verifica viabilidad ‚Üí acepta cualquier objetivo\")\n",
    "print(\"   - No tiene memoria ‚Üí cada plan es independiente\")\n",
    "print(\"\\nüöÄ Mejoras posibles:\")\n",
    "print(\"   - Usar modelos m√°s grandes (GPT-4, Claude)\")\n",
    "print(\"   - A√±adir validaci√≥n de viabilidad\")\n",
    "print(\"   - Implementar memoria de acciones previas\")\n",
    "print(\"   - Permitir replanning si una acci√≥n falla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Conclusiones y Recursos\n",
    "\n",
    "### üéì Resumen del Aprendizaje\n",
    "\n",
    "¬°Felicidades! Has completado un recorrido completo por la evoluci√≥n de los modelos de lenguaje.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Lo que has aprendido:\n",
    "\n",
    "#### **1. Fundamentos Hist√≥ricos**\n",
    "- Evoluci√≥n del NLP: reglas ‚Üí estad√≠stica ‚Üí deep learning ‚Üí transformers ‚Üí LLMs\n",
    "- Cada era resolvi√≥ limitaciones de la anterior\n",
    "\n",
    "#### **2. T√©cnicas Cl√°sicas de NLP**\n",
    "- ‚úÖ Tokenizaci√≥n y normalizaci√≥n de texto\n",
    "- ‚úÖ Stemming vs Lematizaci√≥n\n",
    "- ‚úÖ N-gramas para capturar contexto local\n",
    "- ‚úÖ An√°lisis de frecuencias\n",
    "\n",
    "#### **3. Representaciones Vectoriales**\n",
    "- ‚úÖ Bag of Words (BoW): frecuencias simples\n",
    "- ‚úÖ TF-IDF: ponderaci√≥n por importancia\n",
    "- ‚úÖ Limitaciones: no capturan sem√°ntica\n",
    "\n",
    "#### **4. Embeddings Modernos**\n",
    "- ‚úÖ Word2Vec, GloVe: primeros embeddings sem√°nticos\n",
    "- ‚úÖ Sentence Transformers: embeddings de frases completas\n",
    "- ‚úÖ Similitud del coseno para comparar significados\n",
    "- ‚úÖ Aplicaciones: b√∫squeda sem√°ntica, clustering, clasificaci√≥n\n",
    "\n",
    "#### **5. Arquitecturas Transformer**\n",
    "- ‚úÖ **Encoder (BERT)**: Comprensi√≥n bidireccional ‚Üí Clasificaci√≥n, NER\n",
    "- ‚úÖ **Decoder (GPT)**: Generaci√≥n autoregresiva ‚Üí Chatbots, escritura\n",
    "- ‚úÖ **Encoder-Decoder (T5)**: Transformaci√≥n ‚Üí Traducci√≥n, res√∫menes\n",
    "\n",
    "#### **6. Modelos Generativos**\n",
    "- ‚úÖ Predicci√≥n del siguiente token\n",
    "- ‚úÖ Par√°metros: temperature, top-k, top-p, max_length\n",
    "- ‚úÖ Balance creatividad vs coherencia\n",
    "\n",
    "#### **7. Prompt Engineering**\n",
    "- ‚úÖ Principios: contexto, rol, formato, ejemplos, restricciones\n",
    "- ‚úÖ Zero-shot, one-shot, few-shot learning\n",
    "- ‚úÖ Chain-of-Thought reasoning\n",
    "\n",
    "#### **8. RAG (Retrieval Augmented Generation)**\n",
    "- ‚úÖ Pipeline: b√∫squeda ‚Üí aumentaci√≥n ‚Üí generaci√≥n\n",
    "- ‚úÖ Reduce alucinaciones usando fuentes reales\n",
    "- ‚úÖ Permite actualizar conocimiento sin reentrenar\n",
    "\n",
    "#### **9. Agentes de IA**\n",
    "- ‚úÖ Componentes: percepci√≥n, planificaci√≥n, ejecuci√≥n\n",
    "- ‚úÖ Herramientas y acciones\n",
    "- ‚úÖ Autonom√≠a y toma de decisiones\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Habilidades T√©cnicas Adquiridas\n",
    "\n",
    "**Librer√≠as y frameworks:**\n",
    "- ‚úÖ `spaCy`: Procesamiento ling√º√≠stico\n",
    "- ‚úÖ `NLTK`: An√°lisis de texto cl√°sico\n",
    "- ‚úÖ `scikit-learn`: Vectorizaci√≥n y ML\n",
    "- ‚úÖ `transformers`: Modelos pre-entrenados\n",
    "- ‚úÖ `sentence-transformers`: Embeddings sem√°nticos\n",
    "- ‚úÖ `matplotlib`: Visualizaci√≥n\n",
    "\n",
    "**Conceptos de ML/DL:**\n",
    "- ‚úÖ Embeddings y espacios vectoriales\n",
    "- ‚úÖ Similitud del coseno\n",
    "- ‚úÖ Reducci√≥n dimensional (PCA)\n",
    "- ‚úÖ Clustering (K-means)\n",
    "- ‚úÖ Clasificaci√≥n de texto\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Pr√≥ximos Pasos\n",
    "\n",
    "#### **1. Profundiza en Fundamentos**\n",
    "```python\n",
    "# Temas a explorar:\n",
    "- Arquitectura de Transformers en detalle\n",
    "- Mecanismo de atenci√≥n (self-attention)\n",
    "- Tokenizaci√≥n avanzada (BPE, WordPiece)\n",
    "- Fine-tuning de modelos\n",
    "```\n",
    "\n",
    "#### **2. Proyectos Pr√°cticos**\n",
    "```\n",
    "Nivel Principiante:\n",
    "‚Üí Clasificador de sentimientos para reviews de productos\n",
    "‚Üí Sistema de b√∫squeda sem√°ntica para documentos\n",
    "‚Üí Chatbot con reglas + modelo generativo\n",
    "\n",
    "Nivel Intermedio:\n",
    "‚Üí Sistema RAG para documentaci√≥n t√©cnica\n",
    "‚Üí Resumidor autom√°tico de art√≠culos\n",
    "‚Üí Extractor de entidades nombradas (NER)\n",
    "\n",
    "Nivel Avanzado:\n",
    "‚Üí Fine-tuning de modelos para dominio espec√≠fico\n",
    "‚Üí Multi-agente con LangChain\n",
    "‚Üí Sistema de QA con m√∫ltiples fuentes\n",
    "```\n",
    "\n",
    "#### **3. Frameworks y Herramientas**\n",
    "\n",
    "**LangChain** (https://langchain.com)\n",
    "```python\n",
    "# Framework para aplicaciones con LLMs\n",
    "- Chains: Secuencias de operaciones\n",
    "- Agents: Sistemas aut√≥nomos con herramientas\n",
    "- Memory: Gesti√≥n de contexto conversacional\n",
    "- VectorStores: Bases de datos vectoriales\n",
    "```\n",
    "\n",
    "**LlamaIndex** (https://llamaindex.ai)\n",
    "```python\n",
    "# Especializado en RAG e indexaci√≥n\n",
    "- Indexaci√≥n eficiente de documentos\n",
    "- Query engines avanzados\n",
    "- Integraci√≥n con m√∫ltiples fuentes\n",
    "```\n",
    "\n",
    "**Haystack** (https://haystack.deepset.ai)\n",
    "```python\n",
    "# Framework end-to-end para NLP\n",
    "- Pipelines de b√∫squeda\n",
    "- Question Answering\n",
    "- Generaci√≥n aumentada\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Recursos Recomendados\n",
    "\n",
    "#### **Documentaci√≥n Oficial**\n",
    "\n",
    "| Recurso | URL | Descripci√≥n |\n",
    "|---------|-----|-------------|\n",
    "| Hugging Face Docs | https://huggingface.co/docs | Modelos, datasets, tutoriales |\n",
    "| Transformers Docs | https://huggingface.co/docs/transformers | API de transformers |\n",
    "| Sentence Transformers | https://www.sbert.net | Embeddings de frases |\n",
    "| spaCy | https://spacy.io | NLP industrial |\n",
    "| LangChain | https://python.langchain.com | Aplicaciones con LLMs |\n",
    "\n",
    "#### **Cursos Online (Gratuitos)**\n",
    "\n",
    "**1. Hugging Face Course**  \n",
    "https://huggingface.co/course  \n",
    "- Completo y pr√°ctico\n",
    "- Desde b√°sico hasta avanzado\n",
    "- Certificado gratuito\n",
    "\n",
    "**2. Fast.ai NLP**  \n",
    "https://www.fast.ai/  \n",
    "- Enfoque top-down (pr√°ctica primero)\n",
    "- C√≥digo en PyTorch\n",
    "\n",
    "**3. DeepLearning.AI Short Courses**  \n",
    "https://www.deeplearning.ai/short-courses/  \n",
    "- Cursos de 1-2 horas\n",
    "- Temas espec√≠ficos (RAG, Agents, Fine-tuning)\n",
    "- Gratuitos\n",
    "\n",
    "**4. Stanford CS224N**  \n",
    "http://web.stanford.edu/class/cs224n/  \n",
    "- Curso universitario completo\n",
    "- Videos, slides, tareas\n",
    "- Nivel acad√©mico riguroso\n",
    "\n",
    "#### **Papers Fundamentales**\n",
    "\n",
    "**Deben leer:**\n",
    "\n",
    "1. **\"Attention Is All You Need\"** (2017)  \n",
    "   https://arxiv.org/abs/1706.03762  \n",
    "   ‚Üí Paper original de Transformers\n",
    "\n",
    "2. **\"BERT: Pre-training of Deep Bidirectional Transformers\"** (2018)  \n",
    "   https://arxiv.org/abs/1810.04805  \n",
    "   ‚Üí BERT y masked language modeling\n",
    "\n",
    "3. **\"Language Models are Few-Shot Learners\"** (GPT-3, 2020)  \n",
    "   https://arxiv.org/abs/2005.14165  \n",
    "   ‚Üí Capacidades de modelos grandes\n",
    "\n",
    "4. **\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\"** (2020)  \n",
    "   https://arxiv.org/abs/2005.11401  \n",
    "   ‚Üí RAG original\n",
    "\n",
    "#### **Libros**\n",
    "\n",
    "**1. \"Natural Language Processing with Transformers\"**  \n",
    "por Lewis Tunstall, Leandro von Werra, Thomas Wolf  \n",
    "- Pr√°ct ico y actualizado\n",
    "- C√≥digo con Hugging Face\n",
    "\n",
    "**2. \"Speech and Language Processing\"**  \n",
    "por Dan Jurafsky y James H. Martin  \n",
    "- Gratuito online: https://web.stanford.edu/~jurafsky/slp3/\n",
    "- Fundamentos s√≥lidos\n",
    "\n",
    "**3. \"Dive into Deep Learning\"**  \n",
    "https://d2l.ai  \n",
    "- Interactivo (Jupyter notebooks)\n",
    "- Gratuito\n",
    "\n",
    "#### **Comunidades y Foros**\n",
    "\n",
    "- **Hugging Face Forum**: https://discuss.huggingface.co/\n",
    "- **r/MachineLearning**: https://reddit.com/r/MachineLearning\n",
    "- **r/LanguageTechnology**: https://reddit.com/r/LanguageTechnology\n",
    "- **Discord de Hugging Face**: https://hf.co/join/discord\n",
    "- **Papers With Code**: https://paperswithcode.com/area/natural-language-processing\n",
    "\n",
    "#### **Blogs y Newsletters**\n",
    "\n",
    "- **The Batch** (deeplearning.ai): https://www.deeplearning.ai/the-batch/\n",
    "- **Hugging Face Blog**: https://huggingface.co/blog\n",
    "- **Distill**: https://distill.pub/\n",
    "- **Sebastian Ruder's Blog**: https://ruder.io/\n",
    "- **Jay Alammar's Blog**: https://jalammar.github.io/\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
